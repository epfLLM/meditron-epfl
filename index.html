<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MEDITRON-70B: Scaling Medical Pretraining for Large Language Models">
  <meta name="keywords" content="Meditron, Foundation Model, Generative Modeling, Generative AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MEDITRON-70B</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBVBTHC8QT"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-GBVBTHC8QT');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <!-- For metadata visualizations -->
  <script src="https://d3js.org/d3.v3.min.js"></script>
  <script src="https://unpkg.com/crossfilter@1.3.11/crossfilter.js"></script>
  <script src="./static/js/metadata_designer.js" defer></script>

  <script src="./static/js/index.js"></script>

</head>
<body>
  <nav class="navbar is-transparent is-fixed-top glass-overlay" role="navigation" aria-label="main navigation" id="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="#">
        Home
      </a>

      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu" id="navMenu">
      <div class="navbar-start">
        <a class="navbar-item" href="#about">
          Overview
        </a>
        <a class="navbar-item" href="#publications">
          Papers & code
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#generation">
            Methodology
          </a>

          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#any-to-any">
              Data Mixture
            </a>
            <a class="navbar-item" href="#rgb-to-all">
              Pre-training
            </a>
            <a class="navbar-item" href="#rgb-to-all">
              Evaluation
            </a>

          </div>
        </div>

      </div>

    </div>
  </nav>

<section class="hero headline">

  <div class="hero-body">

    <div class="container is-max-desktop headline-container">

      <div class="columns is-centered">

        <div class="columns has-text-left" style="margin-right: 10%;">
          <div class="title-container">
            <img src="static/images/meditron_LOGO_monogram_transparent.png" class="logo" width="100" alt="4M logo"/>
            <h1 id="scrollingHead" class="title is-1"><b>MEDITRON-70B:</b><br><span class="publication-title">Scaling Medical Pretraining<br>for Large Language Models</span></h1>
          </div>
        </div>
        <div class="logo-container">
          <img src="static/images/epfl-logo.png" class="logo" width="140" alt="EPFL logo"/>
        </div>

      </div>

      <div class="columns is-centered">
        <div class="column has-text-left banner">
          <blockquote>
            A set of new open-access LLMs adapted to the medical domain,<br>achieving new SoTA open-source performance on common medical benchmarks.
          </blockquote>
        </div>
      </div>

      <div class="column has-text-justified">
        <div class="publication-links">
          <!-- PDF Link. -->
          <span class="link-block-header">
            <a href="https://arxiv.org/abs/2311.16079"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="ai ai-arxiv"></i>
              </span>
              <span>Paper</span>
            </a>
          </span>
          <!-- Code Link. -->
          <span class="link-block-header">
            <a href="https://github.com/epfLLM/meditron"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          <span class="link-block-header">
            <a href="https://huggingface.co/epfl-llm"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-database"></i>
              </span>
              <span>Dataset</span>
            </a>
          </span>
        </div>
      </div>

    </div>


  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/medical_performance.png" alt="4M pull figure."/>
      <h2 class="subtitle has-text-centered">
        <span class="dmethod">MEDITRON-70B</span>’s performance on MedQA MEDITRON-70B achieves an accuracy of
        70.2 % on USMLE-style questions in the MedQA (4 options) dataset.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="about">Summary</h2>
        <div class="content has-text-justified">

          <p>
            Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities.
          </p>
          <p>
            In this work, we improve access to large-scale medical LLMs by releasing <span class="dmethod">MEDITRON</span>: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. <span class="dmethod">MEDITRON</span> builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines.
          </p>
          <p>
            Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, <span class="dmethod">MEDITRON</span> achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, <span class="dmethod">MEDITRON-70B</span> outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2.
          </p>
          <p>
            We release our code for curating the medical pretraining corpus and the <span class="dmethod">MEDITRON-70B</span> model weights to drive open-source development of more capable medical LLMs.
          </p>
          <!-- <p>
            <span class="dmethod">4M</span> leads to models that exhibit several key capabilities:
            <ol type="I">
              <li>
                they can perform a diverse set of vision tasks out of the box,
              </li>
              <li>
                they excel when fine-tuned for unseen downstream tasks or new input modalities, and
              </li>
              <li>
                they can function as a generative model that can be conditioned on arbitrary modalities,
                enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.
              </li>
            </ol>
          </p> -->

        </div>

        <figure class="image mod-figure">
          <img src="static/images/meditron-process.png" alt="Modalities overview">
        </figure>
        <!-- <h2 class="subtitle has-text-centered">
            <span class="dmethod">4M</span> enables training a single model on tens
            of diverse modalities. The resulting model can generate <em>any</em> of
            the modalities from <em>any subset</em> of them.
        </h2> -->

      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Introductory video -->
    <div class="responsive-iframe">
      <iframe src="https://www.youtube.com/embed/37ZQI_-17TI?si=1KTdYxHHNc-9aD0O" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
    </div>

    <!-- <h2 class="subtitle has-text-centered">
      Introductory video (6min) <i class="fa fa-volume-up"></i>.
      English captions are available in the video settings.
    </h2> -->
    <!--/ Introductory video -->


    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <a class="anchor" id="publications"></a>
        <h2 class="title is-3">Papers & code</h2>

        <!-- Publication tiles. -->
        <div class="tile is-ancestor is-centered">

          <!-- NeurIPS -->
          <div class="tile is-parent is-12">
            <article class="tile is-child box">
              <p class="title is-4">MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</p>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="" target="_blank">Zeming Chen</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Alejandro Hernández Cano</a> <sup>1 *</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Angelika Romanou</a> <sup>1 *</sup>,
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>EPFL</span>
                &nbsp;
                <span class="author-block"><sup>2</sup>Apple</span>
              </div>

              <div class="is-size-6 publication-contribution">
                <span class="author-block">* Equal contribution</span>
              </div>

              <div class="is-size-5 has-text-weight-semibold mt-3 publication-title">
                <span class="author-block">NeurIPS 2023 (spotlight)</span>
              </div>

              <br>

              <p>
                This paper introduces the <span class="dmethod">4M</span> framework for training
                multimodal and multitask models and applies it to a diverse set of tokenized modalities
                including text, images, geometric, and semantic modalities, as well as neural network
                feature maps. We investigate the capabilities of <span class="dmethod">4M</span> models
                through a series of transfer experiments, and study key design choices in an extensive ablation.
              </p>

              <br>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2312.06647"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a target="_blank" class="external-link button is-normal is-rounded is-dark" disabled>
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Code & weights (coming soon)</span>
                      </a>
                  </span>
                  <!-- Poster -->
                  <span class="link-block">
                    <a href="https://storage.googleapis.com/four_m_site/presentation/4M_Poster_NeurIPS.pdf"
                       target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="far fa-sticky-note"></i>
                      </span>
                      <span>Poster</span>
                      </a>
                  </span>
                  <!-- Slides (Keynote) -->
                  <span class="link-block">
                    <a href="https://storage.googleapis.com/four_m_site/presentation/4M_Slides_NeurIPS.key"
                       target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-file-powerpoint"></i>
                      </span>
                      <span>Slides (Keynote)</span>
                      </a>
                  </span>

                  <!-- BibTeX -->
                  <div id="modal-js-example" class="modal">
                    <div class="modal-background"></div>
                    <div class="modal-content">
                      <div class="box">
                        <h2 class="title">BibTeX</h2>
                          <pre style="text-align: left;"><code>@inproceedings{4m,
  title={{4M}: Massively Multimodal Masked Modeling},
  author={Mizrahi, David and Bachmann, Roman and Kar, O{\u{g}}uzhan Fatih and Yeo, Teresa and Gao, Mingfei and Dehghan, Afshin and Zamir, Amir},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}</code></pre>
                      </div>
                    </div>
                    <button class="modal-close is-large" aria-label="close"></button>
                  </div>
                  <span class="link-block">
                    <a target="_blank" class="external-link button is-normal is-rounded is-dark js-modal-trigger"
                       data-target="modal-js-example">
                      <span class="icon">
                          <i class="fas fa-newspaper"></i>
                      </span>
                      <span>BibTeX</span>
                      </a>
                  </span>

                </div>
              </div>

            </article>
          </div>
          <!--/ NeurIPS -->
        </div>

        <!--/ Publication tiles. -->

      </div>
    </div>

    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Overview</h2>

        <div class="tile is-ancestor is-centered">

          <div class="tile is-parent">
            <a href="#method" class="tile is-child box asdf">
              <p class="title is-4"><span class="dmethod">Data Mixture</p>
              <p>
                Discover the <span class="dmethod">4M</span> training
                strategy and multimodal generation scheme.
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#retrieval" class="tile is-child box">
              <p class="title is-4">Pre-Training</p>
              <p>
                See how <span class="dmethod">4M</span> can distill contrastive objectives
                and emerge multimodal retrieval capabilities.
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#transfers-ablations" class="tile is-child box">
              <p class="title is-4">Evaluation</p>
              <p>
                Explore key design choices and how <span class="dmethod">4M</span>
                transfers to novel downstream tasks.
              </p>
            </a>
          </div>

        </div>

      </div>
    </div>
    <!--/ Overview. -->

  </div>
</section>


<!-- 4M method description, tokenization & modalities. -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- 4M training and generation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <a class="anchor" id="method"></a>
        <h2 class="title is-3"><span class="dmethod">4M</span> method overview</h2>

        <div class="content has-text-justified">
          <p>
            <span class="dmethod">4M</span> is a multimodal and multitask training scheme,
            enabling the training of versatile <i>any-to-any</i> models, i.e. capable of predicting / generating
            any modality from any subset of other modalities.
            <span class="dmethod">4M</span> models are capable of performing a wide variety of vision
            tasks out-of-the-box, and excel when fine-tuned to novel downstream tasks.
          </p>
        </div>

        <a class="anchor" id="training"></a>
        <h2 class="title is-4"><span class="dmethod">4M</span> training</h2>

        <div class="content has-text-justified">
          <p>
            By tokenizing modalities into sequences of discrete tokens, we can train a single unified
            Transformer encoder-decoder on a diverse set of modalities, including text, images,
            geometric, and semantic modalities, as well as neural network feature maps.
            <span class="dmethod">4M</span> is trained by mapping one random subset of tokens to another.
            Please see the animated visualization below for an overview of the <span class="dmethod">4M</span>
            training scheme.
          </p>
        </div>

        <div class="hero-body">
          <div class="field has-addons is-pulled-right" id="play-controls"
               title="Hint: Right click the video and choose 'Show All Controls' to enable more fine-grained video controls.">
            <p class="control">
              <button class="button is-small is-rounded" onclick="playPauseVideo('pullFigVideo')">
                <span class="icon is-small">
                  &nbsp;<i class="fa fa-play"></i>&nbsp;<i class="fa fa-pause"></i>&nbsp;
                </span>
              </button>
            </p>
            <p class="control">
              <button class="button is-small is-rounded" onclick="restartVideo('pullFigVideo')">
                <span class="icon is-small">
                  <i class="fas fa-redo"></i>
                </span>
                <span>Restart animation</span>
              </button>
            </p>
          </div>
          <video id="pullFigVideo" height="100%" width="100%" preload="metadata"
                 title="Hint: Right click the video and choose 'Show All Controls' to enable more fine-grained video controls.">
            <source src="https://storage.googleapis.com/four_m_site/videos/4M_method_figure.mp4#t=25.9"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            (Left): <span class="dmethod">4M</span> is a framework for training multimodal and multitask models that
            operate on tokenized versions of multiple image-like modalities (such as RGB, depth, etc.)
            and sequence modalities (such as captions and bounding boxes). (Right): The <span class="dmethod">4M</span> training
            objective consists of training a Transformer encoder-decoder to predict a randomly selected
            subset of tokens, which is sampled from all modalities, based on another random subset of tokens.
          </h2>
        </div>

        <a class="anchor" id="chained-generation"></a>
        <h2 class="title is-4">Multimodal chained generation</h2>

        <div class="content has-text-justified">
          <p>
            The trained <span class="dmethod">4M</span> models can be used to generate any modality from any combination
            of other modalities, and are able to perform prediction from partial inputs.
            When predicting multiple modalities from one, rather than predicting each individually, <span class="dmethod">4M</span>
            can be used to predict them one-by-one, always looping fully generated modalities back into the input and conditioning
            the generation of subsequent modalities on them. The consequence is that all training modalities can
            be predicted in a self-consistent manner. Please see the animation below for an illustration on how multimodal
            chained generation is performed with <span class="dmethod">4M</span>.
          </p>
        </div>

        <div class="hero-body">
          <div class="field has-addons is-pulled-right" id="play-controls"
               title="Hint: Right click the video and choose 'Show All Controls' to enable more fine-grained video controls.">
            <p class="control">
              <button class="button is-small is-rounded" onclick="playPauseVideo('chainGenVideo')">
                <span class="icon is-small">
                  &nbsp;<i class="fa fa-play"></i>&nbsp;<i class="fa fa-pause"></i>&nbsp;
                </span>
              </button>
            </p>
            <p class="control">
              <button class="button is-small is-rounded" onclick="restartVideo('chainGenVideo')">
                <span class="icon is-small">
                  <i class="fas fa-redo"></i>
                </span>
                <span>Restart animation</span>
              </button>
            </p>
          </div>
          <video id="chainGenVideo" height="100%" width="100%" preload="metadata"
                 title="Hint: Right click the video and choose 'Show All Controls' to enable more fine-grained video controls.">
            <source src="https://storage.googleapis.com/four_m_site/videos/4M_chained_generation.mp4#t=32.5"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            This simplified example illustrates the generation of a
            full RGB image from a partial RGB and bounding box input using the MaskGIT decoding scheme,
            followed by autoregressive generation of a caption. Note that through chaining (i.e. using
            fully generated modalities as conditioning when generating subsequent modalities), we can
            predict multiple modalities in a self-consistent manner. This is in contrast to independently
            generating each modality from the original conditioning, where each generated output is
            consistent with the <em>input</em> but not necessarily with <em>other outputs</em>.
            Generated tokens can be turned back into images, text, and other modalities, using the detokenizers.
          </h2>
        </div>


<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            TODO: Description about tokenizers reconstructions @OZ-->
<!--          </p>-->
<!--        </div>-->

            <!-- Tokenization examples -->
<!--        <div class="tok-panel">-->
<!---->
<!--          <div class="columns is-multiline is-mobile">-->
<!--            <div class="column is-4 has-text-centered">-->
<!--              <div id="tok-gt" class="rgb-image">-->
<!--                <img src="static/images/loading.jpg"/>-->
<!--              </div>-->
<!--              <h2 class="tok-subtitle">Ground Truth</h2>-->
<!--            </div>-->
<!--            <div class="column is-4 has-text-centered">-->
<!--              <div id="tok-vqvae" class="rgb-image">-->
<!--                <img src="static/images/loading.jpg"/>-->
<!--              </div>-->
<!--              <h2 class="tok-subtitle">VQ-VAE</h2>-->
<!--            </div>-->
<!--            <div class="column is-4 has-text-centered">-->
<!--              <div id="tok-vqgan-lpips" class="rgb-image">-->
<!--                <img src="static/images/loading.jpg"/>-->
<!--              </div>-->
<!--              <h2 class="tok-subtitle">VQ-GAN (LPIPS)</h2>-->
<!--            </div>-->
<!--            <div class="column is-4 has-text-centered">-->
<!--              <div id="tok-vqgan-clip" class="rgb-image">-->
<!--                <img src="static/images/loading.jpg"/>-->
<!--              </div>-->
<!--              <h2 class="tok-subtitle">VQ-GAN (CLIP)</h2>-->
<!--            </div>-->
<!--            <div class="column is-4 has-text-centered">-->
<!--              <div id="tok-diffusion" class="rgb-image">-->
<!--                <img src="static/images/loading.jpg"/>-->
<!--              </div>-->
<!--              <h2 class="tok-subtitle">Diffusion model</h2>-->
<!--            </div>-->
<!--            <div class="column is-4 has-text-centered">-->
<!--              <div id="tok-controlnet" class="rgb-image">-->
<!--                <img src="static/images/loading.jpg"/>-->
<!--              </div>-->
<!--              <h2 class="tok-subtitle">ControlNet</h2>-->
<!--            </div>-->
<!--          </div>-->
<!--          -->
<!--          <div class="columns is-centered is-mobile is-vcentered">-->
<!--            <div class="column is-1 has-text-centered"></div>-->
<!--            <div class="column is-10 has-text-centered">-->
<!--              <!-- <br> -->
<!--              <button class="button" id="prev-button" onclick="prevTokImage()">-->
<!--                <span class="icon">-->
<!--                  <i class="fas fa-arrow-left"></i>-->
<!--                </span>-->
<!--                <span>Prev.</span>-->
<!--              </button>-->
<!--              <button class="button" id="next-button" onclick="nextTokImage()">-->
<!--              <span>Next</span>-->
<!--              <span class="icon">-->
<!--                  <i class="fas fa-arrow-right"></i>-->
<!--              </span>-->
<!--              </button>-->
<!--            </div>-->
<!--            <div class="column is-1 has-text-centered"></div>-->
<!--          </div>-->
<!---->
<!--          <p style="text-align:center"><i>-->
<!--            Hint: Use the buttons to explore different images.-->
<!--          </i></p>-->
<!--          <br/>-->
<!---->
<!--        </div>-->
<!--        <!--/ Tokenization examples -->

      </div>
    </div>
    <!--/ 4M training and generation. -->

  </div>
</section>



<hr>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies website template</a>,
            which is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
