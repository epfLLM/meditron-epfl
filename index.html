<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MEDITRON-70B: Scaling Medical Pretraining for Large Language Models">
  <meta name="keywords" content="Meditron, Foundation Model, Medical LLM, Generative AI, AI4Health">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MEDITRON-70B</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBVBTHC8QT"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-GBVBTHC8QT');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/meditron_LOGO_monogram_transparent.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <!-- For metadata visualizations -->
  <script src="https://d3js.org/d3.v3.min.js"></script>
  <script src="https://unpkg.com/crossfilter@1.3.11/crossfilter.js"></script>
  <script src="./static/js/metadata_designer.js" defer></script>

  <script src="./static/js/index.js"></script>

</head>
<body>
  <nav class="navbar is-transparent is-fixed-top glass-overlay" role="navigation" aria-label="main navigation" id="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="#">
        Home
      </a>

      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu" id="navMenu">
      <div class="navbar-start">
        <a class="navbar-item" href="#about">
          Overview
        </a>
        <a class="navbar-item" href="#publications">
          Papers & code
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#generation">
            Methodology
          </a>

          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#any-to-any">
              Data Mixture
            </a>
            <a class="navbar-item" href="#rgb-to-all">
              Pre-training
            </a>
            <a class="navbar-item" href="#rgb-to-all">
              Evaluation
            </a>

          </div>
        </div>

      </div>

    </div>
  </nav>

<section class="hero headline">

  <div class="hero-body">

    <div class="container is-max-desktop headline-container">
      <div class="columns">

      <div class="columns is-centered" style="margin-right: 10%;">>
        <div class="columns has-text-left">
          <div class="title-container" style="padding-left: 3%;">
            <img src="static/images/meditron_LOGO_monogram_transparent.png" class="logo" width="7%" alt="Meditron logo"/>
            <h1 id="scrollingHead" class="title is-1"><b>MEDITRON-70B:</b></h1>
          </div>
        </div>
        <!-- <div class="logo-container">
          <img src="static/images/epfl-logo.png" class="logo" width="20%" alt="EPFL logo"/>
        </div> -->
        <br>
      </div>

      <div class="columns has-text-left">
        <div class="title-container" style="padding: 1%;">
          <h1 id="scrollingHead" class="title is-1"><span class="publication-title">Scaling Medical Pretraining for Large Language Models</span></h1>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column has-text-left banner" style="padding: 1%;">
          <blockquote>
            A set of new open-access LLMs adapted to the medical domain, achieving new SoTA open-source performance on common medical benchmarks.
          </blockquote>
        </div>
      </div>

      <div class="column has-text-justified">
        <div class="publication-links">
          <!-- PDF Link. -->
          <span class="link-block-header">
            <a href="https://arxiv.org/abs/2311.16079"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="ai ai-arxiv"></i>
              </span>
              <span>Paper</span>
            </a>
          </span>
          <!-- Code Link. -->
          <span class="link-block-header">
            <a href="https://github.com/epfLLM/meditron"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          <span class="link-block-header">
            <a href="https://huggingface.co/epfl-llm"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-brain"></i>
              </span>
              <span>Weights</span>
            </a>
          </span>
          <span class="link-block-header">
            <a href="https://huggingface.co/datasets/epfl-llm/guidelines"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-database"></i>
              </span>
              <span>Dataset</span>
            </a>
          </span>
        </div>
      </div>

    </div>


  </div>
</section>

<section class="hero is-light">
  <div class="section" id="org-banners" style="display:flex;">
    <a href="https://www.epfl.ch/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/epfl-logo.png">
    </a>
    <a href="https://www.idiap.ch/en/" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/idiap-logo.png">
    </a>
    <a href="https://open-assistant.io/bye" target="blank" class="ext-link">
      <img class="center-block org-banner" src="static/images/oa-logo.svg" width="100%">
  </a>
    <a href="https://www.yalemedicine.org/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/yale-logo.png">
    </a>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/medical_performance.png" alt="4M pull figure."/>
      <h2 class="subtitle has-text-centered">
        <span class="dmethod">MEDITRON-70B</span>’s performance on common medical reasoning benchmarks: MEDITRON-70B achieves an avergae accuracy of
        72 %. MEDITRON outperforms open-access medical LLMs, GPT-3.5, Med-PaLM, and is in range of the most powerful commercial LLMs--GPT-4 and Med-PaLM-2.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="about">Summary</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities.
          </p>
          <p>
            In this work, we improve access to large-scale medical LLMs by releasing <span class="dmethod">MEDITRON</span>: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. <span class="dmethod">MEDITRON</span> builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines.
          </p>
          <p>
            Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, <span class="dmethod">MEDITRON</span> achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, <span class="dmethod">MEDITRON-70B</span> outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2.
          </p>
          <p>
            We release our code for curating the medical pretraining corpus and the <span class="dmethod">MEDITRON-70B</span> model weights to drive open-source development of more capable medical LLMs.
          </p>
        </div>

        <figure class="image mod-figure">
          <img src="static/images/meditron-process.png" alt="Modalities overview">
        </figure>

        <!-- <h2 class="subtitle has-text-centered">
            <span class="dmethod">4M</span> enables training a single model on tens
            of diverse modalities. The resulting model can generate <em>any</em> of
            the modalities from <em>any subset</em> of them.
        </h2> -->

      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Introductory video -->
    <div class="responsive-iframe">
      <iframe src="https://www.youtube.com/embed/37ZQI_-17TI?si=1KTdYxHHNc-9aD0O" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <a class="anchor" id="publications"></a>
        <h2 class="title is-3">Papers & code</h2>

        <!-- Publication tiles. -->
        <div class="tile is-ancestor is-centered">

          <!-- Journal -->
          <div class="tile is-parent is-12">
            <article class="tile is-child box">
              <p class="title is-4">MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</p>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://eric11eca.github.io" target="_blank">Zeming Chen</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Alejandro Hernández-Cano</a> <sup>1 *</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=2gTrqa4AAAAJ&hl=en" target="_blank">Angelika Romanou</a> <sup>1 *</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="" target="_blank">Antoine Bonnet</a> <sup>1 *</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.idiap.ch/~kmatoba/" target="_blank">Kyle Matoba</a> <sup>2 *</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Francesco Salvi</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://mpagli.github.io/" target="_blank">Matteo Pagliardini</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://olivia-fsm.github.io/" target="_blank">Simin Fan</a> <sup>1</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://www.xamla.com/" target="_blank">Andreas Köpf</a> <sup>3</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=YT1udC0AAAAJ&hl=en" target="_blank">Amirkeivan Mohtashami</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://ch.linkedin.com/in/alexandre-sallinen-033359294?trk=organization_guest_main-feed-card-text" target="_blank">Alexandre Sallinen</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Alireza Sakhaeirad</a> <sup>1</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://ch.linkedin.com/in/vinitra" target="_blank">Vinitra Swamy</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://krawczuk.eu/" target="_blank">Igor Krawczuk</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://bayazitdeniz.github.io/" target="_blank">Deniz Bayazit</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/axel-marmet/?originalSubdomain=ch" target="_blank">Axel Marmet</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://smontariol.github.io/" target="_blank">Syrielle Montariol</a> <sup>1</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://medicine.yale.edu/profile/mary-anne-hartley/" target="_blank">Mary-Anne Hartley</a> <sup>1,4</sup>,
                </span>
                <span class="author-block">
                  <a href="https://people.epfl.ch/martin.jaggi" target="_blank">Martin Jaggi</a> <sup>1 †</sup>,
                </span>
                <span class="author-block">
                  <a href="https://atcbosselut.github.io/" target="_blank">Antoine Bosselut</a> <sup>1 †</sup>,
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>EPFL</span>
                &nbsp;
                <span class="author-block"><sup>2</sup>Idiap Research Institute</span>
                &nbsp;
                <span class="author-block"><sup>3</sup>Open Assistant</span>
                &nbsp;
                <span class="author-block"><sup>4</sup>Yale</span>
              </div>

              <div class="is-size-6 publication-contribution">
                <span class="author-block">* Equal contribution</span> <span class="author-block">† Equal supervision</span>
              </div>

              <div class="is-size-5 has-text-weight-semibold mt-3 publication-title">
                <span class="author-block">Top ML Papers of the Week (by dair.ai)</span>
              </div>

              <br>

              <p>
                <span class="dmethod">MEDITRON</span> is a suite of open-source medical Large Language Models (70B & 7B)
                adapted to the medical domain from Llama-2
                through continued pretraining on a comprehensively curated medical corpus,
                including selected PubMed articles, abstracts, and internationally-recognized medical guidelines.
                <span class="dmethod">MEDITRON</span>-70B,
                finetuned on relevant training data, demonstrates high-level medical reasoning and
                improved domain-specific benchmark performance over Llama-2-70B, GPT-3.5, and Flan-PaLM.
                We democratize an optimized workflow to scale up domain-specific pretraining for medical LLMs
                to help revolutionize access to medical knowledge and evidence through open-source LLMs.
              </p>

              <br>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2312.06647"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/epfLLM/meditron"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Main Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/epfLLM/Megatron-LLM"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Trainer Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/epfl-llm"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-brain"></i>
                      </span>
                      <span>Weights</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/epfl-llm/guidelines"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>
                  <!-- BibTeX -->
                  <div id="modal-js-example" class="modal">
                    <div class="modal-background"></div>
                    <div class="modal-content">
                      <div class="box">
                        <h2 class="title">BibTeX</h2>
                          <pre style="text-align: left;">
                            <code>
                              @misc{
                                chen2023meditron70b,
                                title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models},
                                author={Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
                                year={2023},
                                eprint={2311.16079},
                                archivePrefix={arXiv},
                                primaryClass={cs.CL}
                              }

                              @software{
                                epfmedtrn,
                                author = {Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
                                title = {MediTron-70B: Scaling Medical Pretraining for Large Language Models},
                                month = November,
                                year = 2023,
                                url = {https://github.com/epfLLM/meditron}
                              }
                          </code>
                        </pre>
                      </div>
                    </div>
                    <button class="modal-close is-large" aria-label="close"></button>
                  </div>
                  <span class="link-block">
                    <a target="_blank" class="external-link button is-normal is-rounded is-dark js-modal-trigger"
                       data-target="modal-js-example">
                      <span class="icon">
                          <i class="fas fa-newspaper"></i>
                      </span>
                      <span>BibTeX</span>
                      </a>
                  </span>

                </div>
              </div>

            </article>
          </div>
          <!--/ NeurIPS -->
        </div>

        <!--/ Publication tiles. -->

      </div>
    </div>

    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Overview</h2>

        <div class="tile is-ancestor is-centered">

          <div class="tile is-parent">
            <a href="#data_mixture" class="tile is-child box asdf">
              <p class="title is-4"><span class="dmethod">Data Mixture</p>
              <p>
                Discover <span class="dmethod">MEDITRON</span>'s pre-training data mixture and
                the strategy of data collection & preprocessing.
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#retrieval" class="tile is-child box">
              <p class="title is-4">Pre-Training</p>
              <p>
                 Find out how <span class="dmethod">MEDITRON</span> scales up the continued pre-training in the medical domain for LLMs.
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#transfers-ablations" class="tile is-child box">
              <p class="title is-4">Evaluation</p>
              <p>
                Explore key achievements made by <span class="dmethod">MEDITRON</span>, compared to other state-of-the-art LLMs.
              </p>
            </a>
          </div>

        </div>

      </div>
    </div>
    <!--/ Overview. -->

  </div>
</section>

<!-- Meditron data mixture collection, preprocessing, and details. -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Meditron data mixture -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <a class="anchor" id="data_mixture"></a>
        <h2 class="title is-3"><span class="dmethod">MEDITRON</span> pre-training data mixture overview</h2>

        <div class="content has-text-justified">
          <p>
            <span class="dmethod">MEDITRON</span>'s continued pre-training data mixture is consist of three parts:
            (1) open-access PubMed & PubMed-Central papers and abstracts,
            (2) internationally-recognized clinical guidelines and
            (3) generalist text data from the RedPajama general pre-training data mixture.
            The pre-training data mixture contatins a total number of 48.1B tokens.
          </p>
        </div>

        <a class="anchor" id="training"></a>
        <h2 class="title is-4">Data Collection</h2>

        <div class="content has-text-justified">
          <p>
            By tokenizing modalities into sequences of discrete tokens, we can train a single unified
            Transformer encoder-decoder on a diverse set of modalities, including text, images,
            geometric, and semantic modalities, as well as neural network feature maps.
            <span class="dmethod">4M</span> is trained by mapping one random subset of tokens to another.
            Please see the animated visualization below for an overview of the <span class="dmethod">4M</span>
            training scheme.
          </p>
        </div>

        <div class="hero-body">
          <figure class="image mod-figure">
            <img src="static/images/meditron_data.png" alt="meditron data overview">
          </figure>
          <br>
          <h2 class="subtitle has-text-justified">
            <span class="dmethod">MEDITRON</span>'s pre-training corpus GAP-REPLAY.
            (1): Clinical Guidelines: 46K clinical practice guidelines from various
            healthcare-related sources (2) PubMed Paper & Abstracts: 5M full-text open-access papers from
            PubMed & PubMed Central; public available abstracts from 16.1M closed-access PubMed and PubMed Central papers.
            (3): Replay dataset: general domain text data sampled from RedPajama to compose 1% of the entire corpus.
          </h2>
        </div>

        <a class="anchor" id="chained-generation"></a>
        <h2 class="title is-4">Data Preprocessing</h2>

        <div class="content has-text-justified">
          <p>
            The trained <span class="dmethod">4M</span> models can be used to generate any modality from any combination
            of other modalities, and are able to perform prediction from partial inputs.
            When predicting multiple modalities from one, rather than predicting each individually, <span class="dmethod">4M</span>
            can be used to predict them one-by-one, always looping fully generated modalities back into the input and conditioning
            the generation of subsequent modalities on them. The consequence is that all training modalities can
            be predicted in a self-consistent manner. Please see the animation below for an illustration on how multimodal
            chained generation is performed with <span class="dmethod">4M</span>.
          </p>
        </div>

        <div class="hero-body">
          <div class="field has-addons is-pulled-right" id="play-controls"
               title="Hint: Right click the video and choose 'Show All Controls' to enable more fine-grained video controls.">
            <p class="control">
              <button class="button is-small is-rounded" onclick="playPauseVideo('chainGenVideo')">
                <span class="icon is-small">
                  &nbsp;<i class="fa fa-play"></i>&nbsp;<i class="fa fa-pause"></i>&nbsp;
                </span>
              </button>
            </p>
            <p class="control">
              <button class="button is-small is-rounded" onclick="restartVideo('chainGenVideo')">
                <span class="icon is-small">
                  <i class="fas fa-redo"></i>
                </span>
                <span>Restart animation</span>
              </button>
            </p>
          </div>
          <video id="chainGenVideo" height="100%" width="100%" preload="metadata"
                 title="Hint: Right click the video and choose 'Show All Controls' to enable more fine-grained video controls.">
            <source src="https://storage.googleapis.com/four_m_site/videos/4M_chained_generation.mp4#t=32.5"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            This simplified example illustrates the generation of a
            full RGB image from a partial RGB and bounding box input using the MaskGIT decoding scheme,
            followed by autoregressive generation of a caption. Note that through chaining (i.e. using
            fully generated modalities as conditioning when generating subsequent modalities), we can
            predict multiple modalities in a self-consistent manner. This is in contrast to independently
            generating each modality from the original conditioning, where each generated output is
            consistent with the <em>input</em> but not necessarily with <em>other outputs</em>.
            Generated tokens can be turned back into images, text, and other modalities, using the detokenizers.
          </h2>
        </div>

      </div>
    </div>
  </div>
</section> 

<section class="section" id="training">
  <div class="container is-max-desktop content">
    <h2 class="title">Pre-Training</h2>
    <div class="content has-text-justified">
      <p>
        
      </p>
    </div>
  </div>
</section> 

<section class="section" id="evaluation">
  <div class="container is-max-desktop content">
    <h2 class="title">Evaluation</h2>
    <div class="content has-text-justified">
      <p>
        
      </p>
    </div>
  </div>
</section> 

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@misc{chen2023meditron70b,
  title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models},
  author={Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
  year={2023},
  eprint={2311.16079},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}</code></pre>
  </div>
</section>

<hr>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies website template</a>,
            which is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
