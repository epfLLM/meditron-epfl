<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MEDITRON-70B: Scaling Medical Pretraining for Large Language Models">
  <meta name="keywords" content="Meditron, Foundation Model, Medical LLM, Generative AI, AI4Health">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MEDITRON-70B</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GBVBTHC8QT"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-GBVBTHC8QT');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/meditron_LOGO_monogram_transparent.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <!-- For metadata visualizations -->
  <script src="https://d3js.org/d3.v3.min.js"></script>
  <script src="https://unpkg.com/crossfilter@1.3.11/crossfilter.js"></script>
  <script src="./static/js/metadata_designer.js" defer></script>

  <script src="./static/js/index.js"></script>

</head>
<body>
  <nav class="navbar is-transparent is-fixed-top glass-overlay" role="navigation" aria-label="main navigation" id="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="#">
        Home
      </a>

      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu" id="navMenu">
      <div class="navbar-start">
        <a class="navbar-item" href="#about">
          Overview
        </a>
        <a class="navbar-item" href="#publications">
          Papers & code
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#generation">
            Methodology
          </a>

          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#data_mixture">
              Data Mixture
            </a>
            <a class="navbar-item" href="#pretraining">
              Pre-training
            </a>
            <a class="navbar-item" href="#evaluation">
              Evaluation
            </a>

          </div>
        </div>

      </div>

    </div>
  </nav>

<section class="hero headline">

  <div class="hero-body">

    <div class="container is-max-desktop headline-container">

      <div class="columns">

        <div class="columns has-text-left" style="margin-right: 10%; margin-top: 3%;margin-left: 2%; margin-bottom: 2%;">
          <div class="title-container">
            <img src="static/images/meditron_LOGO_monogram_transparent.png" class="logo" width="100" alt="4M logo"/>
            <h1 id="scrollingHead" class="title is-1"><b>MEDITRON-70B:</b><br><span class="publication-title">Scaling Medical Pretraining<br>for Large Language Models</span></h1>
          </div>
        </div>
        <!-- <div class="logo-container">
          <img src="static/images/epfl-logo.png" class="logo" width="140" alt="EPFL logo"/>
        </div> -->

      </div>

      <div class="columns is-centered">
        <div class="column has-text-left banner">
          <blockquote>
            A set of new open-access LLMs adapted to the medical domain,<br>achieving new SoTA open-source performance on common medical benchmarks.
          </blockquote>
        </div>
      </div>

      <div class="column has-text-justified">
        <div class="publication-links">
          <!-- PDF Link. -->
          <span class="link-block-header">
            <a href="https://arxiv.org/abs/2311.16079"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="ai ai-arxiv"></i>
              </span>
              <span>Paper</span>
            </a>
          </span>
          <!-- Code Link. -->
          <span class="link-block-header">
            <a href="https://github.com/epfLLM/meditron"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          <span class="link-block-header">
            <a href="https://huggingface.co/epfl-llm"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-brain"></i>
              </span>
              <span>Weights</span>
            </a>
          </span>
          <span class="link-block-header">
            <a href="https://huggingface.co/datasets/epfl-llm/guidelines"
              target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-database"></i>
              </span>
              <span>Dataset</span>
            </a>
          </span>
        </div>
      </div>

    </div>


  </div>
</section>

<section class="hero is-light">
  <div class="section" id="org-banners" style="display:flex;">
    <a href="https://www.epfl.ch/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/epfl-logo.png">
    </a>
    <a href="https://www.idiap.ch/en/" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/idiap-logo.png">
    </a>
    <a href="https://open-assistant.io/bye" target="blank" class="ext-link">
      <img class="center-block org-banner" src="static/images/oa-logo.svg">
  </a>
    <a href="https://www.yalemedicine.org/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/yale-logo.png">
    </a>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/medical_performance.png" alt="Meditron performance."/>
      <h2 class="subtitle has-text-centered" style="margin-top: 1%;">
        <span class="dmethod">MEDITRON-70B</span>’s performance on common medical reasoning benchmarks: MEDITRON-70B achieves an avergae accuracy of
        72 %. MEDITRON outperforms open-access medical LLMs, GPT-3.5, Med-PaLM, and is in range of the most powerful commercial LLMs--GPT-4 and Med-PaLM-2.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="about">Summary</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities.
          </p>
          <p>
            In this work, we improve access to large-scale medical LLMs by releasing <span class="dmethod">MEDITRON</span>: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. <span class="dmethod">MEDITRON</span> builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines.
          </p>
          <p>
            Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, <span class="dmethod">MEDITRON</span> achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, <span class="dmethod">MEDITRON-70B</span> outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2.
          </p>
          <p>
            We release our code for curating the medical pretraining corpus and the <span class="dmethod">MEDITRON-70B</span> model weights to drive open-source development of more capable medical LLMs.
          </p>
        </div>

        <div style="margin-top: 5%;">
          <figure class="image mod-figure">
            <img src="static/images/meditron-process.png" alt="Modalities overview">
          </figure>
        </div>


        <!-- <h2 class="subtitle has-text-centered">
            <span class="dmethod">4M</span> enables training a single model on tens
            of diverse modalities. The resulting model can generate <em>any</em> of
            the modalities from <em>any subset</em> of them.
        </h2> -->

      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Introductory video -->
    <div class="responsive-iframe">
      <iframe src="https://www.youtube.com/embed/37ZQI_-17TI?si=1KTdYxHHNc-9aD0O" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <a class="anchor" id="publications"></a>
        <h2 class="title is-3">Papers & code</h2>

        <!-- Publication tiles. -->
        <div class="tile is-ancestor is-centered">

          <!-- Journal -->
          <div class="tile is-parent is-12">
            <article class="tile is-child box">
              <p class="title is-4">MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</p>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://eric11eca.github.io" target="_blank">Zeming Chen</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Alejandro Hernández-Cano</a> <sup>1 *</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=2gTrqa4AAAAJ&hl=en" target="_blank">Angelika Romanou</a> <sup>1 *</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="" target="_blank">Antoine Bonnet</a> <sup>1 *</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.idiap.ch/~kmatoba/" target="_blank">Kyle Matoba</a> <sup>2 *</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Francesco Salvi</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://mpagli.github.io/" target="_blank">Matteo Pagliardini</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://olivia-fsm.github.io/" target="_blank">Simin Fan</a> <sup>1</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://www.xamla.com/" target="_blank">Andreas Köpf</a> <sup>3</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=YT1udC0AAAAJ&hl=en" target="_blank">Amirkeivan Mohtashami</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://ch.linkedin.com/in/alexandre-sallinen-033359294?trk=organization_guest_main-feed-card-text" target="_blank">Alexandre Sallinen</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="" target="_blank">Alireza Sakhaeirad</a> <sup>1</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://ch.linkedin.com/in/vinitra" target="_blank">Vinitra Swamy</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://krawczuk.eu/" target="_blank">Igor Krawczuk</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://bayazitdeniz.github.io/" target="_blank">Deniz Bayazit</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/axel-marmet/?originalSubdomain=ch" target="_blank">Axel Marmet</a> <sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://smontariol.github.io/" target="_blank">Syrielle Montariol</a> <sup>1</sup>,
                </span>
                <br>
                <span class="author-block">
                  <a href="https://medicine.yale.edu/profile/mary-anne-hartley/" target="_blank">Mary-Anne Hartley</a> <sup>1,4</sup>,
                </span>
                <span class="author-block">
                  <a href="https://people.epfl.ch/martin.jaggi" target="_blank">Martin Jaggi</a> <sup>1 †</sup>,
                </span>
                <span class="author-block">
                  <a href="https://atcbosselut.github.io/" target="_blank">Antoine Bosselut</a> <sup>1 †</sup>,
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>EPFL</span>
                &nbsp;
                <span class="author-block"><sup>2</sup>Idiap Research Institute</span>
                &nbsp;
                <span class="author-block"><sup>3</sup>Open Assistant</span>
                &nbsp;
                <span class="author-block"><sup>4</sup>Yale</span>
              </div>

              <div class="is-size-6 publication-contribution">
                <span class="author-block">* Equal contribution</span> <span class="author-block">† Equal supervision</span>
              </div>

              <div class="is-size-5 has-text-weight-semibold mt-3 publication-title">
                <span class="author-block">Top ML Papers of the Week (by dair.ai)</span>
              </div>

              <br>

              <p>
                <span class="dmethod">MEDITRON</span> is a suite of open-source medical Large Language Models (70B & 7B)
                adapted to the medical domain from Llama-2
                through continued pretraining on a comprehensively curated medical corpus,
                including selected PubMed articles, abstracts, and internationally-recognized medical guidelines.
                <span class="dmethod">MEDITRON</span>-70B,
                finetuned on relevant training data, demonstrates high-level medical reasoning and
                improved domain-specific benchmark performance over Llama-2-70B, GPT-3.5, and Flan-PaLM.
                We democratize an optimized workflow to scale up domain-specific pretraining for medical LLMs
                to help revolutionize access to medical knowledge and evidence through open-source LLMs.
              </p>

              <br>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2312.06647"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/epfLLM/meditron"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Main Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/epfLLM/Megatron-LLM"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Trainer Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/epfl-llm"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-brain"></i>
                      </span>
                      <span>Weights</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/epfl-llm/guidelines"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>
                  <!-- BibTeX -->
                  <div id="modal-js-example" class="modal">
                    <div class="modal-background"></div>
                    <div class="modal-content">
                      <div class="box">
                        <h2 class="title">BibTeX</h2>

                          <pre style="text-align: left;">
                            <code>
@misc{
  chen2023meditron70b,
  title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models},
  author={Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
  year={2023},
  eprint={2311.16079},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@software{
  epfmedtrn,
  author = {Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
  title = {MediTron-70B: Scaling Medical Pretraining for Large Language Models},
  month = November,
  year = 2023,
  url = {https://github.com/epfLLM/meditron}
}
                          </code>
                        </pre>
                      </div>
                    </div>
                    <button class="modal-close is-large" aria-label="close"></button>
                  </div>
                  <span class="link-block">
                    <a target="_blank" class="external-link button is-normal is-rounded is-dark js-modal-trigger"
                       data-target="modal-js-example">
                      <span class="icon">
                          <i class="fas fa-newspaper"></i>
                      </span>
                      <span>BibTeX</span>
                      </a>
                  </span>

                </div>
              </div>

            </article>
          </div>
          <!--/ NeurIPS -->
        </div>

        <!--/ Publication tiles. -->

      </div>
    </div>

    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Overview</h2>

        <div class="tile is-ancestor is-centered">

          <div class="tile is-parent">
            <a href="#data_mixture" class="tile is-child box asdf">
              <p class="title is-4"><span class="dmethod">Data Mixture</p>
              <p>
                Discover <span class="dmethod">MEDITRON</span>'s pre-training data mixture and
                the strategy of data collection & preprocessing.
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#pretraining" class="tile is-child box">
              <p class="title is-4">Pre-Training</p>
              <p>
                 Find out how <span class="dmethod">MEDITRON</span> scales up the continued pre-training in the medical domain for LLMs.
              </p>
            </a>
          </div>

          <div class="tile is-parent">
            <a href="#evaluation" class="tile is-child box">
              <p class="title is-4">Evaluation</p>
              <p>
                Explore key achievements made by <span class="dmethod">MEDITRON</span>, compared to other state-of-the-art LLMs.
              </p>
            </a>
          </div>

        </div>

      </div>
    </div>
    <!--/ Overview. -->

  </div>
</section>

<!-- Meditron data mixture collection, preprocessing, and details. -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Meditron data mixture -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <a class="anchor" id="data_mixture"></a>
        <h2 class="title is-3"><span class="dmethod">MEDITRON</span> pre-training data mixture overview</h2>

        <div class="content has-text-justified">
          <p>
            <span class="dmethod">MEDITRON</span>'s continued pre-training data mixture is consist of three parts:
            (1) open-access PubMed & PubMed-Central papers and abstracts,
            (2) internationally-recognized clinical guidelines and
            (3) generalist text data from the RedPajama general pre-training data mixture.
            The pre-training data mixture contatins a total number of 48.1B tokens.
          </p>
        </div>

        <a class="anchor" id="data-collection"></a>
        <h2 class="title is-4">Data Collection</h2>

        <div class="content has-text-justified">
          <p>
            For the medical papers, we include all the open access PubMed papers from the <a target="_blank" href="https://github.com/allenai/s2orc">S2ORC</a> project provided by AI2.
            This includes 4.5M full-text papers from the PubMed Central Open Access Subset.
            In addition, we include 400k open-access PubMed papers that are not found in the PubMed Central archive.
            This gives us a total of 4.9M medical papers. We also include around 15.7 million abstracts from PubMed papers whose full text are not publicly available.
            <br><br>
            For the clinical guidelines, we select based on pragmatic selection criteria.
            We target those that are (1) open-access, (2) systematically formatted with homogenous textual structure
            (i.e., in a format such that automated processes could be deployed without excessive risk of misaligning textual sequences),
            (3) in the language predominantly represented by the pre-training corpus of Llama (i.e., English), and
            (4) covering a breadth of medical sub-domains, audiences (clinician, nurse, patient), and resource settings (high, low, and humanitarian response settings).

            The guideline covers a broad range of contexts. For instance, the geographic scope ranges from global (WHO) to national (CDC, NICE) and regional (Ontario, Melbourne) to institutional (ICRC, Mayo Clinic).
            The corpus also represents health care concerns from high- (Ontario, Melbourne), low- (WHO), and volatile- (ICRC) resource settings.
            The peer review processes also ranged from UN bodies (WHO), institutional review boards (ICRC), professional associations (AAFP) to publicly crowdsourced knowledge bases (WikiDoc).
            <br><br>
            We include a subset of general-domain text. Because we are injecting a large amount of medical knowledge into the model, part of its general knowledge from the original pre-training can be overwritten.
            The general-domain text serves as experience replay to help the model recover some general knowledge during the continued pre-training process.
            We sample tokens from the RedPajama corpus which consists of different types of general-domain text such as wiki, papers, books, web, and code.
          </p>
        </div>

        <div class="hero-body">
          <figure class="image mod-figure" style="padding: 0% 10%;">
            <img src="static/images/meditron_data.png" alt="meditron data overview">
          </figure>
          <br>
          <h2 class="subtitle has-text-justified">
            <span class="dmethod">MEDITRON</span>'s pre-training corpus GAP-REPLAY.
            (1): Clinical Guidelines: 46K clinical practice guidelines from various
            healthcare-related sources (2) PubMed Paper & Abstracts: 5M full-text open-access papers from
            PubMed & PubMed Central; public available abstracts from 16.1M closed-access PubMed and PubMed Central papers.
            (3): Replay dataset: general domain text data sampled from RedPajama to compose 1% of the entire corpus.
          </h2>
        </div>

        <a class="anchor" id="data-preprocess"></a>
        <h2 class="title is-4">Data Preprocessing</h2>

        <div class="content has-text-justified">
          <p>
            To preprocess the papers and abstracts, we mainly followed Galactica (<a target="_blank" href="https://arxiv.org/abs/2211.09085">Taylor et al., 2022</a>),
            a prior large mode that were also trained on academic papers.
            Specifically, we removed metadata information and references which don’t contribute to the medical knowledge, and only kept the main text body.
            Tables and figures with crrupted format were also removed.
            For any text that support the main text such as in-line citations, in text figures and table references and formulas, we wrap them with special tokens.
            Finally we remove any URLs and filter out non-English content to keep the training data consistent with the majority of Llama-2’s original pretraining text.

            For the clincial guidelines, we excluded irrelevant or repetitive content that did not contribute to the textual content,
            such as URLs, references, figures, table delimiters, and ill-formatted characters.
            Additionally, the text was standardized to a unified format with indicated section headers, homogenous space separating paragraphs, and normalized lists.
            Finally, all samples were deduplicated using title matching, and articles that were too short or not in English were filtered out.
          </p>
        </div>

        <div class="hero-body">
          <figure class="image mod-figure" style="padding: 0% 15%;">
            <img src="static/images/gap-replay.png" alt="meditron data details">
          </figure>
          <br>
          <h2 class="subtitle has-text-justified">
            GAP-Replay data mixture statistics. The size of both training and validation sets of the GAP-REPLAY pre-training mixture.
            For each set, we give the total number of samples and the total number of tokens belonging to each dataset.
            The portion of each dataset allocated to the validation set (relative to the training set) is given as a percentage.
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Meditron data mixture -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <a class="anchor" id="pretraining"></a>
        <h2 class="title is-3"><span class="dmethod">MEDITRON</span> Pre-Training Framework</h2>

        <div class="content has-text-justified">
          <p>
            We developed the <a target="_blank" href="https://github.com/epfLLM/Megatron-LLM">Megatron-LLM</a> distributed training library, which extends Nvidia’s <a target="_blank", href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> to support the training of three popular open-source LLMs that have recently been released: Llama, Falcon, and Llama-2.
            We use it to pretrain and finetune all <span class="dmethod">MEDITRON</span> models.
            The library supports several forms of complementary parallelism for distributed training, including Data Parallelism (DP – different GPUs process different subsets of the batches),
            Pipeline Parallelism (PP – different GPUs process different layers), Tensor Parallelism (TP – different GPUs process different subtensors for matrix multiplication).
            The library also includes activation recomputation to reduce memory usage at the expense of increased computation times,
            sequence parallelism to exploit further the coordinate-wise independence of batch norm and dropout operations, fused operations, and other modern primitives to help increase training throughput.
            We also added support for FlashAttention (<a target="_blank" href="https://arxiv.org/abs/2205.14135">Dao et al., 2022</a>) and FlashAttention-2 (<a target="_blank", href="https://arxiv.org/abs/2307.08691">Dao, 2023</a>) for more efficient inference and long-context decoding.
            <br><br>
            Below, we show <span class="dmethod">MEDITRON</span>-70B's learning dynamic of the pre-training run.
          </p>
        </div>

        <div class="hero-body">
          <figure class="image mod-figure">
            <img src="static/images/pretrain.png" alt="meditron data overview">
          </figure>
          <br>
          <h2 class="subtitle has-text-justified">
            Training and validation loss during continued pretraining of the <span class="dmethod">MEDITRON</span>-70B model across the number of processed tokens during the pretraining run.
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Meditron data mixture -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <a class="anchor" id="evaluation"></a>
        <h2 class="title is-3"><span class="dmethod">MEDITRON</span> Evaluation</h2>

        <div class="content has-text-justified">
          <p>
            The following table shows the main results of <span class="dmethod">MEDITRON</span> downstream medical task performance against other best-performing open-
            source medical models measured by accuracy. Our models (<span class="dmethod">MEDITRON-7B</span> and <span class="dmethod">MEDITRON-70B</span>),
            the Llama-2 models (7B and 70B), and PMC-Llama-7B are individually finetuned on PubMedQA,
            MedMCQA, and MedQA training sets.
            <br><br>
            The baselines with ∗, i.e., Mistral-7B (instruct version),
            Zephyr-7B-β, Med42-70B, and Clinical-Camel-70B are instruction-tuned, so we do not perform
            further finetuning on the training sets and use the out-of-box model for inference. We use three inference modes for evaluation:
            <ol type="I">
              <li>Top-token selection based on probability</li>
              <li>Chain-of-thought prompting</li>
              <li>Self-Consistency Chain-of-thought prompting (5 branches with 0.8 temperature)</li>
            </ol>
            According to <a href="https://arxiv.org/abs/2306.10070">Tian et al. (2023)</a>, the passing score for humans on USMLE-MedQA is 60.0.
          </p>
        </div>

        <div class="table-container">
          <table class="table is-hoverable table-row-active-background-color=hsl(0%, 100%, 41%)" align="center">
            <thead>
              <tr>
                <th></th>
                <th></th>
                <th></th>
                <th class="has-text-centered">Accuracy(↑)</th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
            </thead>
            <thead>
              <tr>
                <th><span class="dmethod">Model</span></th>
                <th class="has-text-centered">MMLU-Medical</th>
                <th class="has-text-centered">PubMedQA</th>
                <th class="has-text-centered">MedMCQA</th>
                <th class="has-text-centered">MedQA</th>
                <th class="has-text-centered">MedQA-4-Option</th>
                <th class="has-text-centered">Avg</th>
              </tr>
            </thead>
            <thead>
              <tr>
                <th></th>
                <th></th>
                <th class="has-text-centered" colspan="3"><span class="dmethod">Top Token Selection</span></th>
                <th></th>
                <th></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <th>Mistral-7B</td>
                <td class="has-text-centered">55.8</th>
                <td class="has-text-centered">17.8</td>
                <td class="has-text-centered">40.2</td>
                <td class="has-text-centered">32.4</td>
                <td class="has-text-centered">41.1</td>
                <td class="has-text-centered">37.5</td>
              </tr>
              <tr>
                <th>Zephyr-7B-β</td>
                <td class="has-text-centered">63.3</th>
                <td class="has-text-centered">46.0</td>
                <td class="has-text-centered">43.0</td>
                <td class="has-text-centered">42.8</td>
                <td class="has-text-centered">48.5</td>
                <td class="has-text-centered">48.7</td>
              </tr>
              <tr>
                <th>PMC-Llama-7B</th>
                <td class="has-text-centered">59.7</td>
                <td class="has-text-centered">59.2</td>
                <td class="has-text-centered">57.6</td>
                <td class="has-text-centered">42.4</td>
                <td class="has-text-centered">49.2</td>
                <td class="has-text-centered">53.6</td>
              </tr>
              <tr>
                <th>Llama-2-7B</td>
                <td class="has-text-centered">56.3</th>
                <td class="has-text-centered">61.8</td>
                <td class="has-text-centered">54.4</td>
                <td class="has-text-centered">44.0</td>
                <td class="has-text-centered">49.6</td>
                <td class="has-text-centered">53.2</td>
              </tr>
              <tr class="is-selected">
                <th><span class="dmethod">MEDITRON-7B</span></td>
                <td class="has-text-centered">55.6</td>
                <td class="has-text-centered">74.4</td>
                <td class="has-text-centered">59.2</td>
                <td class="has-text-centered">47.9</td>
                <td class="has-text-centered">52.0</td>
                <td class="has-text-centered"><u>57.5</u></td>
              </tr>
              <tr>
                <th class="has-text-centered"></th>
                <td class="has-text-centered"></td>
                <td class="has-text-centered"></td>
                <td class="has-text-centered"></td>
                <td class="has-text-centered"></td>
                <td class="has-text-centered"></td>
                <td class="has-text-centered"></td>
              </tr>
              <tr>
                <th>Clinical-Camel-70B</th>
                <td class="has-text-centered">65.7</td>
                <td class="has-text-centered">67.0 </td>
                <td class="has-text-centered">46.7</td>
                <td class="has-text-centered">50.8</td>
                <td class="has-text-centered">56.8</td>
                <td class="has-text-centered">57.4</td>
              </tr>
              <tr>
                <th>Med42-70B</th>
                <td class="has-text-centered">74.5</td>
                <td class="has-text-centered">61.2</td>
                <td class="has-text-centered">59.2</td>
                <td class="has-text-centered">59.1</td>
                <td class="has-text-centered">63.9</td>
                <td class="has-text-centered">63.6</td>
              </tr>
              <tr>
                <th>Llama-2-70B</th>
                <td class="has-text-centered">74.7</td>
                <td class="has-text-centered">78.0</td>
                <td class="has-text-centered">62.7</td>
                <td class="has-text-centered">59.2</td>
                <td class="has-text-centered">61.3</td>
                <td class="has-text-centered">67.2</td>
              </tr>
              <tr class="is-selected">
                <th><span class="dmethod">MEDITRON-70B</span></td>
                <td class="has-text-centered">73.6</td>
                <td class="has-text-centered">80.0</td>
                <td class="has-text-centered">65.1</td>
                <td class="has-text-centered">60.7</td>
                <td class="has-text-centered">65.4</td>
                <td class="has-text-centered"><u>69.0</u></td>
              </tr>
              <tr>
                <th></th>
                <th></th>
                <th></th>
                <th class="has-text-centered"><span class="dmethod">Chain-of-thought</span></th>
                <th></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th>Llama-2-70B</th>
                <td class="has-text-centered">76.7</td>
                <td class="has-text-centered">79.8</td>
                <td class="has-text-centered">62.1</td>
                <td class="has-text-centered">60.8</td>
                <td class="has-text-centered">63.9</td>
                <td class="has-text-centered">68.7</td>
              </tr>
              <tr class="is-selected">
                <th><span class="dmethod">MEDITRON-70B</span></td>
                <td class="has-text-centered">74.9</td>
                <td class="has-text-centered">81.0</td>
                <td class="has-text-centered">63.2</td>
                <td class="has-text-centered">61.5</td>
                <td class="has-text-centered">67.8</td>
                <td class="has-text-centered"><u>69.7</u></td>
              </tr>
              <tr>
                <th></th>
                <th></th>
                <th class="has-text-centered" colspan="3"><span class="dmethod">Self-consistency Chain-of-thought</span></th>
                <th></th>
                <th></th>
              </tr>
              <tr>
                <th>Llama-2-70B</th>
                <td class="has-text-centered"><b>77.9</b></td>
                <td class="has-text-centered">80.0</td>
                <td class="has-text-centered">62.6</td>
                <td class="has-text-centered">61.5</td>
                <td class="has-text-centered">63.8</td>
                <td class="has-text-centered">69.2</td>
              </tr>
              <tr class="is-selected">
                <th><span class="dmethod">MEDITRON-70B</span></td>
                <td class="has-text-centered">77.6</td>
                <td class="has-text-centered"><b>81.6</b></td>
                <td class="has-text-centered"><b>66.0</b></td>
                <td class="has-text-centered"><b>64.4</b></td>
                <td class="has-text-centered"><b>70.2</b></td>
                <td class="has-text-centered"><b>72.0</b></td>
              </tr>
              </tbody>
            </table>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@misc{chen2023meditron70b,
  title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models},
  author={Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
  year={2023},
  eprint={2311.16079},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}</code></pre>
  </div>
</section>

<hr>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies website template</a>,
            which is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
